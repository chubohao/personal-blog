<!doctype html>
<html lang="en">

    <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="Bohao Chu, and Bootstrap contributors">
    <title>Demand Forecast | Bohao Chu</title>
    <link href="assets/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom style sheet -->
    <link type="text/css" rel="stylesheet" href="assets/css/style.css"/>

    <!-- Google font -->
    <link href="https://fonts.googleapis.com/css?family=Nunito+Sans:700%7CNunito:300,600" rel="stylesheet">

    <!--Title Icon-->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.9.1/font/bootstrap-icons.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <script src="https://code.jquery.com/jquery-3.5.1.min.js"></script>
    </head>


    <body>

        <header class="fixed-top header"></header>

		<main>
			<div class="section px-0">
				<div id="post-header" class="page-header pt-5 py-md-5" style="background-image: url('assets/img/twins/bg.png'); background-color:#4F4F4F; background-position-x: 95%; background-size: auto 85%">
					<div class="container">
						<div class="row">
							<div class="col-md-10 col-sm-11 post-meta">
								<a class="post-category cat-2" href="#">Machine Learning</a>
								<span class="post-date">Oct, 2022 </span>
								<br><br>
								<h1>Supply Chain Demand Forecast</h1>
								<h3 class="text-light pt-3">
									Fusion ARIMA-LSTM model for supply chain demand forecasting
								</h3>
								<p>
									<span class="text-light">Bohao Chu, Yichen Wang</span>
								</p>
								<button type="button" class="btn btn-light btn-lg btn-circle me-3">
                                    <span class="fa fa-file-pdf-o"></span>
                                </button>
								<button type="button" class="btn btn-light btn-lg btn-circle">
                                    <span class="fa fa-graduation-cap"></span>
                                </button>

							</div>
						</div>
					</div>
				</div>
			</div>

			<div class="section">
				<!-- container -->
				<div class="container">
					<!-- row -->
					<div class="row">
						<!-- POST MAIN -->
						<div class="col-md-6 pe-md-4">

							<h3>INTRODUCTION</h3>

							<div class="section-row">
								<figure class="figure-img text-center">
									<img class="img-responsive" src="assets/img/cvae/1.png" alt="" width="90%">
									<figcaption class="my-3">
										<strong>Figure 1.</strong> The relationship between the complexity of <br> networking and deployment and the number of sensors or sensing purposes.
									</figcaption>
								</figure>
							</div>

							<div class="row">
								<p>
									Predicting realistic human movement is an important task to devise safe autonomous robots in society. Human movement is generally influenced by social rules, most importantly avoiding collisions. In order to predict realistic human movement, the predictions need to follow those social rules. We introduce a new generative approach to predict human trajectories that are collision-free, multimodal and realistic, as they achieve high accuracy in densely crowded datasets. We achieve this by using a pooling module, that encodes human interaction and scene context and further enforce our goal by using a collision loss. The second aspect, namely multimodality is important as human movement is stochastic. To generate multimodal outputs our model uses a Conditional Variational Auto Encoder (CVAE). A CVAE can generate different predictions for the same input, by reducing the dimension of the input into a latent space, thus making the predictions multimodal. Based on this our model can predict a variety of possible future steps and also a distribution, leading to safer robot movement planning in application. We achieve concrete future steps for evaluation by sampling from the generated distribution. Many earlier approaches did not take human collision into account making their predictions unrealistic. Our model achieves comparable results in evaluation while also accounting for the important features of multimodality and collision-avoidance in human trajectory prediction.
								</p>

								<h3 class="mt-4">METHOD</h3>
								<div class="row text-center">
									<figure class="figure-img col-12">
										<img class="img-responsive" src="assets/img/cvae/m.jpeg" alt="" width="99%">
										<figcaption>
										<strong>TABLE I.</strong> EVALUATION OF OUR 3 MODEL ADAPTIONS ON ALL SUBDATASETS COMPARED TO BASELINE LSTM. THE RESULTS ARE ADE/FDE/ACT. LOWER IS BETTER AND BOLD INDICATES BEST.
									</figcaption>
									</figure>
								</div>

								<p>
									The presented model is called CoLoss-CVAE. The structure of CoLoss-CVAE is depicted in Figure 2. It consists of an RNN-Encoder-Decoder-Framework, realized by a Long-Short-Term-Memory (LSTM), a Conditional Variational Auto Encoder (CVAE) to predict a conditional distribution and a pooling module to encode human interaction, also called scene context. The inputs to our model are given by OBS and P RED_GT as described above. Both of those inputs are then encoded into LSTM hidden states.
								</p>

								<h6><i>LSTM ENCODER</i></h6>
								<p>
									The LSTM-Encoder autoregressively encodes the history of each agents trajectories into a single LSTM hidden state. There is one LSTM to encode the observed trajectories into the condition c and one LSTM to encode the groundtruth future trajetories into the input x. Both LSTMs work similarly to the described Encoder-LSTM below:
								</p>

								<h6><i>CVAE</i></h6>
								<p>
									A CVAE is used to generate diverse multimodal outputs for the same input, as deterministic approaches would not suffice to represent realistic human behaviour. The goal of a CVAE is to learn the distribution P (y|c) of the output y given the condition c. This is done by using a latent variable z. The latent variable is sampled from latent space Z and the distribution of Z is learned by the CVAE. In training the distribution Q(z|x, c), called the posterior, is learned. This is the distribution of the latent space Z, which the latent variable z is sampled from. The latent space Z is of lower dimension than the input space and is used to encode input features. By encoding the input to a lower space, a one-to- many mapping is achieved in the output when generating a y from given z, making the model multimodal.
								</p>

								<h6><i>POOLING MODULE</i></h6>
								<p>
									The pooling module introduced in [12] encodes social interactions between the current person i and all neighbors j in the scene. It uses the current distance dt i,j between the person i and all their neighbors, as well as their current Decoder-LSTM hidden-states. The Decoder-LSTM is further explained in chapter. The advantage of also using the LSTM hidden-state is, that it also encodes previous movement, as not only the current distance is relevant in collision avoidance. The relation of neighbors is then computed by a Multi-Layer-Perceptron, as follows:
								</p>

							</div>

						</div>

						<!-- POST ASIDE -->
						<div class="col-md-6 ps-md-4">
							<div class="row mb-5">
								<h3 class="mt-4">EXPERIMENT</h3>
								<div class="row text-center">
									<figure class="figure-img col-12">
										<img class="img-responsive" src="assets/img/cvae/t1.png" alt="" width="100%">
										<figcaption>
										<strong>TABLE I.</strong> EVALUATION OF OUR 3 MODEL ADAPTIONS ON ALL SUBDATASETS COMPARED TO BASELINE LSTM. THE RESULTS ARE ADE/FDE/ACT. LOWER IS BETTER AND BOLD INDICATES BEST.
									</figcaption>
									</figure>



									<figure class="figure-img col-12">
										<img class="img-responsive" src="assets/img/cvae/t2.png" alt="" width="100%">
										<figcaption>
										<strong>TABLE II.</strong> EVALUATION OF OUR BEST MODEL ON ALL SUBDATASETS COMPARED TO 4 DIFFERENT BASELINE MODELS. THE RESULTS ARE ADE/FDE/ACT. LOWER IS BETTER AND BOLD INDICATES BEST.
									</figcaption>
									</figure>
								</div>

								<p>
									In this Section, we evaluate our model on two public available datasets ETH [13] and UCY [9]. These two datasets include huge numbers of pedestrian trajectories with inter- actions between multiple people from real world scenarios. These datasets are based on 4 different scenes - ETH & HOTEL (from ETH) and UNIV & ZARA (from UCY), which consist of 1536 pedestrians in total. The datasets are divided into the 5 subdatasets ETH, HOTEL, UNIV, ZARA1 and ZARA2. Each of them is further divided into training set, validation set and test set. We use a version, which is unified by Gupta et al. [13] so that all trajectories are in the same metric world coordinate system and interpolate to obtain the trajetories respective positions at every timestep equal to 0.4s. As same as previous work, the subdataset is evaluated using the leave-one-out approach. Four subdatasets are used for training and the remaining subdataset is used for testing.
								</p>

								<div class="row text-center">
									<figure class="figure-img col-4">
										<img class="img-responsive" src="assets/img/cvae/p1.png" alt="" width="99%">
									</figure>
									<figure class="figure-img col-4">
										<img class="img-responsive" src="assets/img/cvae/p2.png" alt="" width="99%">
									</figure>
									<figure class="figure-img col-4">
										<img class="img-responsive" src="assets/img/cvae/p3.png" alt="" width="99%">
									</figure>

									<figure class="figure-img col-4">
										<img class="img-responsive" src="assets/img/cvae/p4.png" alt="" width="99%">
									</figure>
									<figure class="figure-img col-4">
										<img class="img-responsive" src="assets/img/cvae/p5.png" alt="" width="99%">
									</figure>
									<figure class="figure-img col-4">
										<img class="img-responsive" src="assets/img/cvae/p6.png" alt="" width="99%">
									</figure>
									<figcaption>
										<strong>TABLE I.</strong> EVALUATION OF OUR 3 MODEL ADAPTIONS ON ALL SUBDATASETS COMPARED TO BASELINE LSTM. THE RESULTS ARE ADE/FDE/ACT. LOWER IS BETTER AND BOLD INDICATES BEST.
									</figcaption>

								</div>


								<p>
									First we compared the results of our different models to see which architecture leads to the best results. As a baseline we used a basic LSTM-Encoder-Decoder trained with LN LL and LCOL called LSTM-NLL-Coll. The evaluation is shown in Table I.

								</p>

								<p>
									This evaluation showed that our model CoLoss-CVAE-MMD-N(0,1) using MMD unexpectedly performed worse than our Baseline-LSTM. We implemented it due to the good performance the MMD achieved in [17]. We assume this bad performance resulting from MMD not being suited for distributions in particular, but for stacks of data. The computation of MMD is based on comparing all of the moments in two data stacks. To use MMD it is possible to treat the distributions like data stacks. Nevertheless the implementation in [17] lead to good results, eventually by a different way to implement MMD to learn the prior
								</p>

								<p>
									Our model CoLoss-CVAE-KLD-N(0,1) achieved the best results in average ADE and FDE, while average ACT=0.021 was still comparable to our best result ACT=0.017, that was achieved by CoLoss-CVAE-KLD-PRIOR. This results also were astonishing, as we implemented the Prior-Net to achieve a prior distribution of higher quality as also shown in [17]. As in this model a second, more complex, distribution has to be learned, we trace back the results to this expanded architecture, making the training more com- plex. Nevertheless our model CoLoss-CVAE-KLD-N(0,1) achieved good results, compared in Table II to state-of-the- art models. CoLoss-CVAE will in further context refer to our best performing model CoLoss-CVAE-KLD-N(0,1).
								</p>

								<div class="row text-center">
									<figure class="figure-img col-4">
										<img class="img-responsive" src="assets/img/cvae/e1.png" alt="" width="99%">
									</figure>
									<figure class="figure-img col-4">
										<img class="img-responsive" src="assets/img/cvae/e2.png" alt="" width="99%">
									</figure>
									<figure class="figure-img col-4">
										<img class="img-responsive" src="assets/img/cvae/e3.png" alt="" width="99%">
									</figure>

									<figure class="figure-img col-4">
										<img class="img-responsive" src="assets/img/cvae/e4.png" alt="" width="99%">
									</figure>
									<figure class="figure-img col-4">
										<img class="img-responsive" src="assets/img/cvae/e5.png" alt="" width="99%">
									</figure>
									<figure class="figure-img col-4">
										<img class="img-responsive" src="assets/img/cvae/e6.png" alt="" width="99%">
									</figure>
									<figcaption>
										<strong>TABLE I.</strong> EVALUATION OF OUR 3 MODEL ADAPTIONS ON ALL SUBDATASETS COMPARED TO BASELINE LSTM. THE RESULTS ARE ADE/FDE/ACT. LOWER IS BETTER AND BOLD INDICATES BEST.
									</figcaption>

								</div>
							</div>



						</div>
						<!-- /aside -->
					</div>
					<!-- /row -->
				</div>
				<!-- /container -->
			</div>
			<!-- /section -->
		</main>


        <footer class="section bg-secondary mt-5 footer"></footer>
        <script src="assets/js/bootstrap.bundle.min.js"></script>
        <script>
            addEventListener("DOMContentLoaded", function() {
                $(".header").load("components/header.html",function(){});
                $(".footer").load("components/footer.html",function(){});
            });
        </script>

    </body>
</html>
