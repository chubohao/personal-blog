<!doctype html>
<html lang="en">
    <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="Bohao Chu, and Bootstrap contributors">
    <title>ECG | Bohao Chu</title>
    <link href="assets/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom style sheet -->
    <link type="text/css" rel="stylesheet" href="assets/css/style.css"/>

    <!-- Google font -->
    <link href="https://fonts.googleapis.com/css?family=Nunito+Sans:700%7CNunito:300,600" rel="stylesheet">

    <!--Title Icon-->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.9.1/font/bootstrap-icons.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <script src="https://code.jquery.com/jquery-3.5.1.min.js"></script>
	<script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>
	<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/styles/atom-one-dark.min.css">
	<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/highlight.min.js"></script>
	</head>

    <body>

        <header class="fixed-top header"></header>

		<main>
			<div class="section px-0">
				<div id="post-header" class="page-header pt-5" style="background-image: url('assets/img/ecg/bg.png'); background-position-y: 90%; background-position-x: 90%; background-size: auto 80%">
					<div class="container">
						<div class="row">
							<div class="col-md-10 col-sm-11 post-meta">
								<a class="post-category cat-1" href="#">Machine Learning</a>
								<a class="post-category cat-2" href="#">Embedded System</a>
								<span class="post-date">Oct 3, 2022 </span>
								<br><br>
								<h1>Edge Computing Vision Gateway</h1>
								<h3 class="text-light pt-3">
								TPU Accelerated Video Gateway for Visual Computing.
								</h3>
								<p>
									<span class="text-light pt-2">
										Bohao Chu, Chao Wang, and Yangjie Cao
									</span>
								</p>
								<button type="button" class="btn btn-dark btn-lg btn-circle me-3">
                                    <span class="fa fa-file-pdf-o"></span>
                                </button>
								<button type="button" class="btn btn-dark btn-lg btn-circle">
                                    <span class="fa fa-graduation-cap"></span>
                                </button>
							</div>
						</div>
					</div>
				</div>
			</div>

			<div class="section">
				<!-- container -->
				<div class="container">
					<!-- row -->
					<div class="row">
						<!-- POST MAIN -->
						<div class="col-md-8 pe-md-5">
							<h3>INTRODUCTION</h3>
							<div class="section-row">
								<figure class="figure-img text-center">
									<img class="img-responsive" src="assets/img/ecg/arch.png" alt="" width="100%">
									<figcaption class="my-3">
										<strong>Figure 1.</strong> Archtecture of Edge Computing Vision Gateway.
									</figcaption>
								</figure>
							</div>
							<h4>
								BACKGROUND
							</h4>
							<p>
								We know that in the construction site area, construction workers must wear helmets to guard against falling objects. But even though supervisors give this requirement to construction workers, someone always forgets or chooses not to wear a helmet for various reasons, which brings great safety risks to his life. Previously, supervisors often patrolled regularly to detect whether each person was wearing a helmet correctly, but this was time-consuming and labor-intensive, and the supervisors were unable to monitor in real time. Therefore, they want to be able to use visual computing to determine whether construction workers are wearing helmets, and if someone is found not wearing a helmet, a warning will be issued.
							</p>

							<h4>
								MOTIVATION
							</h4>
							<p>
								Object recognition or action recognition based on visual computing has received a lot of attention and made great progress, such as ImageNet, YOLO, and so on. However, this area is still full of many challenges, such as how to improve the speed of video transmission, the speed of model recognition, and the generalization ability of models. This work will focus on these problems and implement a vision gateway based on YOLOv5s Model.
							</p>

							<div class="row">
								

<h3 class="mt-4">METHOD</h3>

<h4>
1. Mlti-camera video concurrent pull stream
</h4>
<p>
This work uses the Hikvision camera that supports RTSP protocol, according to the product manual can be known to pull the video stream as shown in the base code 1. To realize multiple pulling cameras, we use a multi-threaded approach.
</p>

<pre class="hljs card m-2">
<code>
import cv2
# Camera Login Information
ip='192.168.2.111'
user='admin'
password='123456'
# rtsp://[username]:[passwd]@[ip]:[port]/[codec]/[channel]/[subtype]/av_stream
capture = cv2.VideoCapture("rtsp://"+ user +":"+ password +"@" + ip + ":554/h264/ch1/main/av_stream")
ret, frame = capture.read()
cv2.namedWindow(ip, 0)
cv2.resizeWindow(ip, 500, 300)
while ret:
	ret, frame = capture.read()
	cv2.imshow(ip,frame)
	if cv2.waitKey(1) & 0xFF == ord('q'):
		break
cv2.destroyAllWindows()
capture.release()
</code>
</pre>
<p class="px-3 pt-3 text-center">
	<small><strong>Code 1.</strong> Camera Video Stream Pulling Based on RTSP Protocol.</small>
</p>

<h4 class="mt-2">
2. Detect people's body and avatar in video streams
</h4>
<p>
We used some of the detectors that come with in to detect human bodies, and avatars in video frames (pictures). These detectors have been successfully applied to pedestrian detection in still images. They can be directly passed as parameters to the program HaarFaceDetect.
<br>
- Upper body detector (most fun, useful in many scenarios!)
<br>
- Lower body detector
<br>
- Full body detector
</p>


<pre class="hljs card m-2">
<code>
import cv2
cv2.namedWindow("Helmet")
# download from opencv
# haarcascade_fullbody.xml for body
face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')
while True:
	ret, frame = capture.read()
	gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)
	# detect avatar
	faces = face_cascadedetectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(20, 20), flags=cv2.CASCADE_SCALE_IMAGE)
	# lable avatar
	for (x, y, w, h) in faces:
		cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)
		cv2.imshow('Helmet', frame)
		if cv2.waitKey(5) & 0xFF == ord('q'):
			break
</code>
</pre>
<p class="px-3  pt-3 text-center">
	<small><strong>Code 2.</strong> Getting the human body, and avatar region in a video stream.</small>
</p>

<h4 class="mt-2">
3. Helmet Recognition Model based on YOLOv5
</h4>
<a href="https://github.com/njvisionpower/Safety-Helmet-Wearing-Dataset">
	A. Dataset [Safety-Helmet-Wearing-Dataset]
</a>
<p>
	SHWD provide the dataset used for both safety helmet wearing and human head detection. It includes 7581 images with 9044 human safety helmet wearing objects(positive) and 111514 normal head objects(not wearing or negative). The positive objects got from goolge or baidu, and we manually labeld with LabelImg. Some of negative objects got from SCUT-HEAD. We fixed some bugs for original SCUT-HEAD and make the data can be directly loaded as normal Pascal VOC format. Also we provide some pretrained models with MXNet GluonCV.
</p>
<pre class="hljs card m-2">
<code>
path: ../datasets/helmet  # dataset root dir
train: images/train  # train images
val: images/val      # val images

# Classes (3 classes)
names:
	0: person
	1: head
	2: helmet
</code>
</pre>
<p class="px-3  pt-3 text-center">
	<small><strong>Code 3.</strong> Configurate Dataset.</small>
</p>

<a href="https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/#12-create-labels_1">
	B. Create Labels
</a>
<a href="https://jinglingbiaozhu.com/" class="text-primary">We use JingLingBiaoZhu tool to lable our images</a>
<p>
After using an annotation tool to label our images, export our labels to YOLO format, with one *.txt file per image (if no objects in image, no *.txt file is required). The *.txt file specifications are:
<br>
- One row per object
<br>
- Each row is class x_center y_center width height format.
<br>
- Box coordinates must be in normalized xywh format (from 0 - 1). If your boxes are in pixels, divide x_center and width by image width, and y_center and height by image height.
<br>
- Class numbers are zero-indexed (start from 0).
</p>
<pre class="hljs card m-2">
<code>
import numpy as np
def convert(size, box):
	"""
	convert labled [x1,y1,x2,y2] to location using by YOLOv5
	:param size: [w,h]
	:param box: [x1,y1,x2,y2]
	:return: [x,y,w,h]
	"""
	x1 = int(box[0])
	y1 = int(box[1])
	x2 = int(box[2])
	y2 = int(box[3])

	dw = np.float32(1. / int(size[0]))
	dh = np.float32(1. / int(size[1]))

	w = x2 - x1
	h = y2 - y1
	x = x1 + (w / 2)
	y = y1 + (h / 2)

	x = x * dw
	w = w * dw
	y = y * dh
	h = h * dh
	return [x, y, w, h]
</code>
</pre>
<p class="px-3  pt-3 text-center">
	<small><strong>Code 4.</strong> Convert the image to the format that YOLOv5 needs.</small>
</p>







<a href="https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/#13-organize-directories">
	C. Organize Directories
</a>
<p>
	Organize your train and val images and labels according to the example below. YOLOv5 assumes /coco128 is inside a /datasets directory next to the /yolov5 directory. YOLOv5 locates labels automatically for each image by replacing the last instance of /images/ in each image path with /labels/.
</p>
<pre class="hljs card m-2">
<code>
../datasets/helmet/images/im0.jpg  # image
../datasets/helmet/labels/im0.txt  # label
</code>
</pre>
<p class="px-3  pt-3 text-center">
	<small><strong>Code 5.</strong> Organize Directories.</small>
</p>
<a href="https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/#2-select-a-model">
	D. Select a Model
</a>
<p>
	Select a pretrained model to start training from. Here we select YOLOv5s, the second-smallest and fastest model available.
</p>
<figure class="figure-img text-center">
	<img class="img-responsive" src="assets/img/ecg/models.png" alt="" width="100%">
</figure>
<p class="px-3  pt-3 text-center">
	<small><strong>Figure 2.</strong> YOLOv5 models.</small>
</p>


<a href="https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/#3-train">
	E. Train
</a>
<p>
	Train a YOLOv5s model on helmet by specifying dataset, batch-size, image size and either pretrained --weights yolov5s.pt (recommended), or randomly initialized --weights '' --cfg yolov5s.yaml (not recommended). Pretrained weights are auto-downloaded from the latest YOLOv5 release.
</p>

<p>
	All training results are saved to runs/train/ with incrementing run directories, i.e. runs/train/exp2, runs/train/exp3 etc. For more details see the Training section of our tutorial notebook.
</p>

<pre class="hljs card m-2">
<code>
python train.py --img 640 --epochs 3 --data helmet.yaml --weights yolov5s.pt
</code>
</pre>
<p class="px-3  pt-3 text-center">
	<small><strong>Code 6.</strong> Start train model.</small>
</p>

							

								<h3 class="mt-2">
									RESULT
								</h3>

								

								<h3 class="mt-4">
									KEYWORDS
								</h3>
								<p class="text-dark">
									<strong>
										Edge Computing, YOLOv5, CV2, Body Recognition, Face Recognition.
									</strong>
								</p>

							</div>

						</div>

						<!-- POST ASIDE -->
						<div class="col-md-4">
							<div class="row mb-5">
								<h3>MATERIALS</h3>
								<h4>
									About YOLOv5
								</h4>
								<figure class="figure-img text-center">
									<img class="img-responsive" src="assets/img/ecg/models.png" alt="" width="100%">
								</figure>
								<p class="px-3 text-center">
									<small><strong>Figure 3.</strong> YOLOv5 models.</small>
								</p>
								<figure class="figure-img text-center">
									<img class="img-responsive" src="assets/img/ecg/yolo_models.png" alt="" width="101%">
								</figure>
								<p class="px-3 text-center">
									<small><strong>Figure 4.</strong> Comparison of parameters of different models.</small>
								</p>
								<figure class="figure-img text-center">
									<img class="img-responsive" src="assets/img/ecg/yolo5.png" alt="" width="100%">
								</figure>
								<p class="px-3 text-center">
									<small><strong>Figure 5.</strong> Training and effects of different models in YOLOv5.</small>
								</p>
								<figure class="figure-img text-center">
									<img class="img-responsive" src="assets/img/ecg/yolo.png" alt="" width="100%">
								</figure>
								<p class="px-3 text-center">
									<small><strong>Figure 6.</strong> Comparison with others version.</small>
								</p>
								<figure class="figure-img text-center">
									<img class="img-responsive" src="assets/img/ecg/yolo1.png" alt="" width="100%">
								</figure>
								<p class="px-3 text-center">
									<small><strong>Figure 7.</strong> Training result on COCO128 dataset.</small>
								</p>

								<h4>HARDWARE</h4>
								<figure class="figure-img text-center">
									<img class="img-responsive" src="assets/img/ecg/hardware.png" alt="" width="100%">
								</figure>
								<p class="px-3 text-center">
									<small><strong>Figure 8.</strong> Bridging a Wireless NIC to a Wired NIC.</small>
								</p>
								<p>
									We designed a visual gateway based on the Raspberry Pi Computing Module 4 and set the wireless network card to AP mode for connecting the attached camera and connecting to the Internet by bridging with a wired connection.
								</p>
								<p>
									To increase the inference speed of the machine learning model, we added a Google TPU gas pedal with 4TOPS of arithmetic power.
								</p>
								<p>
									At the same time, we added some basic sensors, such as temperature and humidity, motion sensors, GPS, etc. to detect the environment around the gateway, as well as LoRa and 4G capabilities.
								</p>
								

								<figure class="figure-img text-center">
									<img class="img-responsive" src="assets/img/ecg/title.png" alt="" width="100%">
								</figure>
								<p class="px-3 text-center">
									<small><strong>Figure 9.</strong> 3D model of our gateway.</small>
								</p>
								
								<figure class="figure-img text-center">
									<img class="img-responsive" src="assets/img/ecg/gateway-3.png" alt="" width="100%">
								</figure>
								<p class="px-3 text-center">
									<small><strong>Figure 10.</strong> Gateway after soldering.</small>
								</p>

								<h4>TRAINNING</h4>
								<p>
									We used the open source Data Safety-Helmet-Wearing-Dataset. Many thanks to the authors as well as the photo participants. Currently, we just use the training data to train the model briefly, and the effect is as described by the authors all the time, the next step, we will port the model to the web for real-time video recognition.
								</p>
								<div class="row m-0 p-0">
									<img class="img-responsive col-6 py-2" src="assets/img/ecg/5.jpg" alt="" width="100%">
									<img class="img-responsive col-6 py-2" src="assets/img/ecg/5r.jpg" alt="" width="100%">
									<img class="img-responsive col-6 py-2" src="assets/img/ecg/4.jpg" alt="" width="100%">
									<img class="img-responsive col-6 py-2" src="assets/img/ecg/4r.jpg" alt="" width="100%">
									<img class="img-responsive col-6 py-2" src="assets/img/ecg/1.jpg" alt="" width="100%">
									<img class="img-responsive col-6 py-2" src="assets/img/ecg/1r.jpg" alt="" width="100%">
									<img class="img-responsive col-6 py-2" src="assets/img/ecg/2.jpg" alt="" width="100%">
									<img class="img-responsive col-6 py-2" src="assets/img/ecg/2r.jpg" alt="" width="100%">
									<img class="img-responsive col-6 py-2" src="assets/img/ecg/3.jpg" alt="" width="100%">
									<img class="img-responsive col-6 py-2" src="assets/img/ecg/3r.jpg" alt="" width="100%">
								</div>
								<p class="px-3 text-center">
									<small><strong>Figure 11.</strong> Model evaluation results.</small>
								</p>
								<h4>HARDWARE</h4>
								<p>
									TO DO ...
								</p>
								
							</div>
						</div>
						<!-- /aside -->
					</div>
					<!-- /row -->
				</div>
				<!-- /container -->
			</div>
			<!-- /section -->
		</main>


        <footer class="section bg-secondary mt-5 footer"></footer>
        <script src="assets/js/bootstrap.bundle.min.js"></script>
        <script>
			hljs.initHighlightingOnLoad();
            addEventListener("DOMContentLoaded", function() {
                $(".header").load("components/header.html",function(){});
                $(".footer").load("components/footer.html",function(){});
            });
        </script>

    </body>
</html>
